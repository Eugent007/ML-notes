Запустив 2 модели YOLO12n (Detection_of_seagulls_v1 and Detection_of_seagulls_v2) с одинаковыми параметрами epochs=100, batch size = 10, learning rate = 0.001 и разницей в том что у 1 из них было условие для earling stopping, можно сделать вывод что в предтренированной модели yolo условие ранней остановки не является 100% вариантом для улучшения качества, т.к. остановка произошла слишком рано и без неё можель смогла добиться лучших показателей

Запустив модель YOLOv8n в первый раз (Detection_of_seagulls_v3) с параметрами epoochs=100, batch size = 8, lr = 0.001 мною была замечена тенденция к переобучению (по графику потерь на тренировочных и валидационных данных), поэтому было принято решение обучать её на меньшем количестве эпох = 50 (Detection_of_seagulls_v4), и это привело к улучшению качества модели (к моему удивлению YOLO12n при техже параметрах выдал показатели хуже, но тут я предполагаю что для 12 версии нужно больше эпох)

Для YOLOv8n график потерь более зубчатый (но тут возможно не до конца подобраны параметры), а также заметно что тенденция к переобучению проявляется намного ярче чем в YOLO12n

Самые лучшие результаты получились на версии (Detection_of_seagulls_v5): YOLO12n, epochs=150, batch size=10, learning rate = 0.001, patience = default, DROPOUT = 0.1 (для регуляризации, отбрасываем рандомные нейроны)
